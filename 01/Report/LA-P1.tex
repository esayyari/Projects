\listfiles
\documentclass[twoside,12pt]{article}
\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}
\usepackage{hyperref}
\usepackage{enumerate}
\usepackage[top=2in, bottom=1.5in, left=0.85in, right=0.5in]{geometry}
\usepackage[hyphenbreaks]{breakurl}
%\usepackage[pdfstartview=FitH,pdfstartpage=13,pdfpagemode=UseNone]{hyperref}
\usepackage{amsfonts}
\usepackage{graphicx} 
\usepackage[linesnumbered,ruled]{algorithm2e}
\usepackage{float}
\usepackage{amssymb,amsmath}
\usepackage{mdwlist }
\usepackage{color}
\definecolor{darkblue}{rgb}{0.0,0.0,0.5}
\newtheorem{Dfn}{Definition}
\hypersetup{colorlinks,breaklinks,
            linkcolor=darkblue,urlcolor=darkblue,
            anchorcolor=darkblue,citecolor=darkblue}
\newcommand{\sign}{\text{sign}}
\begin{document}

\title{Learning Algorithms, Project 1}
\author{Arya Iranmehr, Mohsen Malmir, Erfan Sayyari}
\maketitle

\section{Introduction}

\section{Design and Analysis of Algorithm}
Objective function
\begin{equation}
LCL=\sum_{i:y_i=1} \log p_i +\sum_{i:y_i=-1} \log (1-p_i)
\label{eq:LCL}
\end{equation}

Partial derivative
\begin{equation}
\nabla_j=\frac{\partial LCL }{\partial \beta_j}LCL=\sum_{i} (y_i-p_i)x_{ij}
\label{eq:pLCL}
\end{equation}

Stochastic gradient $\tilde{\nabla}$ for the stochastic mini-batch $\Omega$
\begin{equation}
\tilde{\nabla}_j=\frac{\partial LCL }{\partial \beta_j}LCL=\sum_{i \in \Omega} (y_i-p_i)x_{ij}
\label{eq:pLCL}
\end{equation}
\subsection{Stochastic Gradient Descent}
The Stochastic Gradient Descent (SGD) method approximates objective function, simply by restricting the training set to a mini-batch, i.e. random sample. 
This makes a huge difference when the training set is large and can not be fitted into memory. We implemented SGD so that the size of mini-batch could be set\footnote{When size of mini-batch is very small, e.g. 1, the stochastic gradient become noisy leading to slow rate of convergence. On the other hand, we can choose to use larger mini-batch size to reduce the variance of the gradient, which makes gradient computation more costly.} to achieve desirable rate of convergence.

\subsection{LBFGS}
We used the {\it fmin\_l\_bfgs\_b} from { \it scipy.optimize} package, which implements the limited memory {\it BFGS} algorithm and is written by Ciyou Zhu, Richard Byrd, and Jorge Nocedal. This function takes as input the function to be minimized, which is in this case the negative of regularized conditional log-likelihood,
\begin{equation}
RLCL = \sum_{i=1}^N y_i \log(P_i) + (1-y_i)\log(1 - P_i) - \mu \|\beta\|_2^2
\end{equation}
where 
\begin{equation}
P_i = \frac{1}{1+e^{-\sum_{j=1}^d \beta_j x^i_j}}
\end{equation}
Is the probability of the $i$th sample $x^i$ belonging to class with label 1. 
\section{Design of Experiments}
\subsection{Initialization}
\subsection{Preprocessing}
\subsection{Grid Search}
\subsection{Performance Measure}

\section{Results of Experiments}
\subsection{Logistic Regression}
\subsection{Regularized Logistic Regression}

\section{Findings and Lessons Learned}
\subsection{Numerical Issues and Preprocessing}
\subsection{Overfitting}
\subsection{Model Selection}
\subsection{Trade-off Between Mini-Batch size and Rate of Convergence}

\end{document}

