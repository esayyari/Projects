\listfiles
\documentclass[twoside,12pt]{article}
\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}
\usepackage{hyperref}
\usepackage{enumerate}
\usepackage[top=2in, bottom=1.5in, left=0.85in, right=0.5in]{geometry}
\usepackage[hyphenbreaks]{breakurl}
%\usepackage[pdfstartview=FitH,pdfstartpage=13,pdfpagemode=UseNone]{hyperref}
\usepackage{amsfonts}
\usepackage{graphicx} 
\usepackage[linesnumbered,ruled]{algorithm2e}
\usepackage{float}
\usepackage{amssymb,amsmath}
\usepackage{mdwlist }
\usepackage{color}
\definecolor{darkblue}{rgb}{0.0,0.0,0.5}
\newtheorem{Dfn}{Definition}
\hypersetup{colorlinks,breaklinks,
            linkcolor=darkblue,urlcolor=darkblue,
            anchorcolor=darkblue,citecolor=darkblue}
\newcommand{\sign}{\text{sign}}
\newcommand{\argmin}{\arg\!\max}
\begin{document}

\title{Learning Algorithms, Project 1}
\author{Arya Iranmehr, Mohsen Malmir, Erfan Sayyari}
\maketitle

\section{Introduction}
In this project, we have implemented Regularized Logistic Regression (RLR) using Stochastic Gradient Descent (SGD) and L-BFGS method. We studied the behavior of RLR on the Gender Recognition dataset \footnote{The Gender Recognition dataset available at \url{http://mlcomp.org/datasets/1571}}. This report is organized as follows: in section 2 we explain the design and analysis of our implementation of RLR, in section 3 we specify the design of experiments and implementation considerations. We will demonstrate our experimental results in section 4, and finally we outline the lessons we have learned from this project in section 5.

\section{Design and Analysis of Algorithm}
In this project, we consider the binary classification problem with the training set of $n$ labeled examples ${\{(x_i,y_i)\}}^{n}_{i=1}$, $y_i \in \{0,1\}$, $w,x \in \mathbb{R}^d$. The logistic regression maximizes Log Conditional Likelihood (LCL) over the training examples:
\begin{equation}
LCL=\sum_{i:y_i=1} \log p_i +\sum_{i:y_i=-1} \log (1-p_i)
\label{eq:LCL}
\end{equation}

which $p_i=P_{Y|X}(1|x_i)=\sigma(w^Tx_i +b)$ are posterior probabilities of the training examples and $\sigma(.)$ is the sigmoid function. 
It is common to augment training examples with an extra dimension and set it to one for all the training examples,  i.e. $x_{i0}=1$ and defined $\beta=(b, w^T)^T$.
Maximizing LCL is a convex optimization problem, therefore the local maximum is indeed global maximum. So we can use gradient based methods to find local solution of the LCL maximization problem. For both SGD and L-BFGS gradient of the LCL is the key information that solver uses to find solution. The partial derivative of LCL is defined as
\begin{equation}
\nabla_j=\frac{\partial LCL }{\partial \beta_j}LCL=\sum_{i} (y_i-p_i)x_{ij}
\label{eq:pLCL}
\end{equation}
 In the next subsections we show how we maximized the LCL using SGD and L-BFGS.

\subsection{Stochastic Gradient Descent}
The SGD method approximates objective function, simply by restricting the training set to a mini-batch, i.e. random sample from training set. 
This makes a huge difference when the training set is large and can not be fitted into memory. We implemented SGD so that the size of mini-batch could be set\footnote{When size of mini-batch is very small, e.g. 1, the stochastic gradient become noisy leading to slow rate of convergence. On the other hand, we can choose to use larger mini-batch size to reduce the variance of the gradient, which makes gradient computation more costly.} to achieve desirable rate of convergence. 

Also, in many applications the training data is not available as a batch, and training examples arrive in advance. Such a paradigm is called \emph{Online Learning} which SGD is the main tool of solving these problems.

Stochastic gradient $\tilde{\nabla}$ for the stochastic mini-batch $\Omega$ is defined
\begin{equation}
\tilde{\nabla}_j=\frac{\partial LCL }{\partial \beta_j}LCL=\sum_{i \in \Omega} (y_i-p_i)x_{ij}
\label{eq:pLCL}
\end{equation}
SGD steps along steepest ascent direction which is the gradient 
\begin{equation}
\beta_j \leftarrow \beta_j+ \lambda \sum_{i \in \Omega} (y_i-p_i)x_{ij}
\end{equation}
which $\lambda$ is the \emph{learning rate} (step-length). It is important to set learning rate to an admissible value (neither too large or too small). Apart from adaptive schemes (Backtracking line search), Heuristics (Levenburg-Marquardt method), we use a simple method for updating learning rate. The algorithm starts with $\lambda_0$ and after each epoch (one pass over training set) it decays the learning rate by a constant factor $\eta$. By using this strategy we are need to find a good value for the hyperparameter $\lambda_0$, using grid search and cross-validation


\subsection{LBFGS}
We used the $fmin\_l\_bfgs\_b$ from $scipy.optimize$ package, which implements the limited memory {\it BFGS} algorithm and is written by Ciyou Zhu, Richard Byrd, and Jorge Nocedal. This function takes as input the function to be minimized, which is in this case the negative of regularized conditional log-likelihood,
\begin{equation}
RLCL = \sum_{i=1}^N y_i \log(P_i) + (1-y_i)\log(1 - P_i) - \mu \|\beta\|_2^2
\end{equation}
where 
\begin{equation}
P_i = \frac{1}{1+e^{-\sum_{j=1}^d \beta_j x^i_j}}
\end{equation}
Is the probability of the $i$th sample $x^i$ belonging to class with label 1. 

<<<<<<< HEAD
\subsection{L-BFGS}
For testing our implementation of training with L-BFGS, we used the gender detection dataset mentioned in the homework. This is a dataset with more than 800 features and less than 600 samples, meaning that there are many features that are redundant. The data files show that the input data for training and testing are both very sparse, this increases the potential for over-fitting in the case of logistic regression. However, as we mentioned before, regularization comes to our help to prevent over-fitting.
\subsubsection{Initialization}
The initialization of the parameters is limited to the weight vector $beta$, which is the decision boundary for logistic regression. An incorrect choice for the initial value would be a vector of all zeros, $[0\; 0\; \ldots\; 0]$.
\subsubsection{Preprocessing}
\subsubsection{Grid Search}
\subsubsection{Performance Measure}
=======
\subsection{Regularization}
The gradient based methods such as SGD and L-BFGS always take step along direction of ascent \footnote{Any vector $s$ which satisfies $s^T\nabla>0$ is kind of direction of ascent.}. This gradient is always positive, since $y_i-p_i \geq 0$ $\forall y_i, p_i$. So after each epoch $\beta_j$ will be greater or equal to the previous $\beta_j$. This implies that the learning algorithm tends to assign either positive or negative large values to $\beta_j$. In this situation the parameters are overtrained for the current training examples, and the performance on the training set is not representative for the unseen examples, since the learning algorithm become \emph{optimistic} about its performance. This phenomena is known as overfitting in machine learning community and the standard measure for it is called regularization. Among all kinds of regularization, we choose $l_2$-norm  regularization because it is
\begin{itemize}
	\item simple to implement and analyze
	\item smooth and convex
	\item computationally inexpensive
\end{itemize}
So, the optimization problems becomes
\begin{equation}
\beta^* = \operatorname*{arg\,max}_{\beta}{LCL - \frac{\mu}{2}\Vert \beta \Vert^2_2}
\end{equation}
which the term $\Vert\beta_j\Vert^2_2=\beta^T\beta$ penalizes $\beta$s with large magnitudes and weight decay $\mu$ is the trade-off hyperparameter between bias (underfiting) and variance (overfitting). Our final update rule for RLR is
\begin{equation}
\beta_j \leftarrow \ \beta_j+\lambda\left((y_i-p_i)x_j-\mu\beta_j\right)
\end{equation}

\subsection{Computational Complexity}
Our implementation of SGD consist of two nested loops. The outer loop counts up to \emph{max-epochs}, which in each of its iterations it takes one pass over training set.
In the inner loop, algorithm selects a mini-batch of size $|\Omega|=k$ from dataset and computes gradient for it which costs $\mathcal{O}(kd)$, provided that we reuse the probability estimates. The inner loop iterates $\frac{n}{k}$ times so the total computational complexity of the algorithm is of $\mathcal{O}(nd)$
\subsection{Implementation}

\section{Design of Experiments}
\subsection{Initialization}
\subsection{Preprocessing}
\subsection{Grid Search}
\subsection{Performance Measure}
>>>>>>> 4144f496c7b580afd51bd42a6d3f799d6b24b911

\section{Results of Experiments}
\subsection{Logistic Regression}
\subsection{Regularized Logistic Regression}

\section{Findings and Lessons Learned}
\subsection{Numerical Issues and Preprocessing}
\subsection{Overfitting}
\subsection{Model Selection}
\subsection{Trade-off Between Mini-Batch size and Rate of Convergence}

\end{document}
